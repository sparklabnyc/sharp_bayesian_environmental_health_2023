---
title: "Hierarchical modelling"
subtitle: "SHARP Bayesian Modeling for Environmental Health Workshop"
author: "Theo Rashid, Elizaveta Semenova"
date: "August 2023"
format: html
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(nimble)
library(bayesplot)
library(posterior)
library(hrbrthemes)
library(sf)
library(colorspace)

extrafont::loadfonts()
theme_set(theme_ipsum())

color_scheme_set(scheme = "viridis")

set.seed(2)
```

## Goal of this computing lab session

This goal of this lab is to introduce hierarchical modelling using the `NIMBLE` modelling framework.

## What's going to happen in this lab session?

During this lab session, we will:

1. Write a hierarchical model in `NIMBLE`;
2. Compare the model to more basic non-hierarchical models; and 
3. Discuss the advantages of hierarchical models.

## Introduction

In this task, we will explore mortality data in Italy during XX-XX using hierarchical modelling.

::: aside
The standardised mortality ratio (SMR) is the ratio of the number of deaths observed in a population over a given period to the number that would be expected over the same period if the study population had the same age-specific rates as the standard population.
:::

We will estimate the standardised mortality ratio (SMR) for populations in different provinces of Italy using three types of models:

1. Full pooling
2. No pooling
3. Partial pooling

::: aside
This workflow (fitting full/no/partial pooling models) is adapted from a classic Bayesian modelling example: the radon model. You can read about this model in Gelman and Hill's _Data Analysis Using Regression and Multilevel/Hierarchical Models_ (2006) or on the [tensorflow probability website](https://www.tensorflow.org/probability/examples/Multilevel_Modeling_Primer).
:::

## Exploratory data analysis

Let's load in the data.
```{r}
data <- read_rds(here("data", "italy", "italy_mortality.rds"))
glimpse(data)
summary(data)
```

Let's collapse the time dimension for now and focus on estimating the death rate in the final year and the 9th month.
```{r}
data <- data |>
  # make dummies for each province, useful for modelling
  mutate(provincia_id = data |> group_by(SIGLA) |> group_indices()) |>
  filter(year == 2018) |>
  filter(month == 9) |>
  arrange(SIGLA)
```

Let's look at the distribution the number of deaths in each province.
```{r}
#| label: fig-deaths-province
#| warning: false
#| fig-height: 10
#| fig-cap: Number of deaths at each time point in each province.

data |>
  ggplot(aes(x = SIGLA, y = deaths)) +
  geom_jitter(size = 0.4, alpha = 0.8, colour = "darkcyan") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 7))
```

Let's plot the mean number of deaths in that month in that province.
```{r}
shp_italy <- read_rds(here("data", "italy", "italy_shp.rds")) |> arrange(SIGLA)

shp_italy |>
  left_join(
    data |>
      group_by(SIGLA) |>
      summarise(mean_deaths = mean(deaths))
  ) |>
  ggplot(aes(fill = mean_deaths)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()
```

We have a variable `expected`, which gives a good (if a slight underestimation) approximation to the number of deaths.
```{r}
data |>
  ggplot(aes(x = expected, y = deaths)) +
  geom_abline(slope = 1, intercept = 0, linewidth = 0.1) +
  geom_point(size = 0.3)
```

```{r}
data |>
  ggplot(aes(x = deaths / expected)) +
  geom_histogram()
```

We are going to estimate the region-specific SMR.
This is the ratio of actual deaths to expected deaths in each region.

## Full pooling model

First, let's treat all regions the same and estimate a single national prevalence.

The model is as follows:

Priors
$$
\alpha \sim N(0, 5)
$$

Likelihood
$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \log(E_i) + \alpha
\end{split}
$$

The parameter $\alpha$ here is common for all observations $y_i$, regardless of what province the observation belongs to.

```{r}
constants <- list(
  N = nrow(data),
  Np = max(data$provincia_id),
  province = data$provincia_id
)

inits <- list(list(alpha = 0), list(alpha = 0.1))

nimble_data <- list(
  y = as.integer(data$deaths),
  E = data$expected
)
```
```{r}
full_pooling_model <- nimbleCode({
  # priors
  alpha ~ dnorm(0, 5)

  # likelihood
  for (i in 1:N) {
    y[i] ~ dpois(mu[i])
    log(mu[i]) <- log(E[i]) + alpha
  }
})
```

```{r}
full_pooling_samples <- nimbleMCMC(
  code = full_pooling_model,
  data = nimble_data,
  constants = constants,
  inits = inits,
  monitors = c("alpha"),
  niter = 2000,
  nburnin = 1000,
  thin = 1,
  nchains = 2,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE,
  progressBar = TRUE
)
```

Follow the Bayesian workflow: look at MCMC outputs, check convergence using r-hat.
```{r}
summarise_draws(full_pooling_samples, default_summary_measures())
summarise_draws(full_pooling_samples, default_convergence_measures())
```

Let's plot the SMR in each province.
```{r}
full_pooling_posterior <- as_draws_array(full_pooling_samples)

SMR_full_pooling <- exp(full_pooling_posterior[, , "alpha"]) |>
  apply(MARGIN = 3, FUN = median)

p_full <- shp_italy |>
  mutate(SMR = SMR_full_pooling) |>
  ggplot(aes(fill = SMR)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()

p_full
```

Pretty boring, eh?

## No pooling model

Here, we treat all regions separately and estimate the SMR in each region separately with fixed effects.

Priors
$$
\alpha_j \sim N(0, 1) \quad j = 1,..., N_p
$$

Likelihood
$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \log(E_i) + \alpha_{j[i]}
\end{split}
$$

There is now an $\alpha$ for each province, so the map will no longer be uniform.

```{r}
inits <- list(
  list(
    alpha = rep(0, constants$Np)
  ),
  list(
    alpha = rep(0.1, constants$Np)
  )
)
```
```{r}
no_pooling_model <- nimbleCode({
  # priors
  for (j in 1:Np) {
    alpha[j] ~ dnorm(0, sd = 1)
  }

  # likelihood
  for (i in 1:N) {
    y[i] ~ dpois(mu[i])
    log(mu[i]) <- log(E[i]) + alpha[province[i]]
  }
})
```

```{r}
no_pooling_samples <- nimbleMCMC(
  code = no_pooling_model,
  data = nimble_data,
  constants = constants,
  inits = inits,
  monitors = c("alpha"),
  niter = 2000,
  nburnin = 1000,
  thin = 1,
  nchains = 2,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE,
  progressBar = TRUE
)
```

```{r}
summarise_draws(no_pooling_samples, default_summary_measures())
summarise_draws(no_pooling_samples, default_convergence_measures())
```

The SMR in each province
```{r}
no_pooling_posterior <- as_draws_array(no_pooling_samples)

SMR_no_pooling <- exp(no_pooling_posterior[, , 1:107]) |>
  apply(MARGIN = 3, FUN = median)

p_no <- shp_italy |>
  mutate(SMR = SMR_no_pooling) |>
  ggplot(aes(fill = SMR)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()

p_no
```

## Partial pooling model

This is also known as a varying intercept model.
We fit a random effect for each province.
The size of the random effect is controlled by the hyperparameter $\sigma_p$.
There is now a hierarchy in the parameters between $\sigma_p$ and the effects for each province $\theta$ â€“ hence, "hierarchical modelling".

Priors
$$
\begin{split}
\alpha &\sim N(0,5), \\
\sigma_p &\sim N^+(1) \\
\theta_j &\sim N(0, \sigma^2_p) \quad j = 1,..., N_p
\end{split}
$$

Likelihood
$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \log(E_i) + \alpha + \theta_{j[i]}
\end{split}
$$

```{r}
inits <- list(
  list(
    alpha = 0,
    theta = rep(0, constants$Np),
    sigma_p = 1
  ),
  list(
    alpha = 0.1,
    theta = rep(0, constants$Np),
    sigma_p = 1
  )
)
```
```{r}
partial_pooling_model <- nimbleCode({
  # priors
  alpha ~ dnorm(0, 5)
  sigma_p ~ T(dnorm(0, 1), 0, Inf) # half-normal

  for (j in 1:Np) {
    theta[j] ~ dnorm(0, sd = sigma_p)
  }

  # likelihood
  for (i in 1:N) {
    y[i] ~ dpois(mu[i])
    log(mu[i]) <- log(E[i]) + alpha + theta[province[i]]
  }
})
```

```{r}
partial_pooling_samples <- nimbleMCMC(
  code = partial_pooling_model,
  data = nimble_data,
  constants = constants,
  inits = inits,
  monitors = c("alpha", "sigma_p", "theta"),
  niter = 2000,
  nburnin = 1000,
  thin = 1,
  nchains = 2,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE,
  progressBar = TRUE
)
```

```{r}
summarise_draws(partial_pooling_samples, default_summary_measures())
summarise_draws(partial_pooling_samples, default_convergence_measures())
```

With more complicated models, sometimes it's nice to see if the traceplots are working too.
```{r}
mcmc_trace(partial_pooling_samples, regex_pars = "alpha")
```

There is a little bit more manipulation required to estimate the SMR for the more complicated model.
```{r}
partial_pooling_posterior <- as_draws_array(partial_pooling_samples)

SMR_partial_pooling <- exp(
  sweep(
    partial_pooling_posterior[, , 3:109],
    partial_pooling_posterior[, , 1],
    MARGIN = c(1, 2),
    FUN = "+"
  )
) |>
  apply(MARGIN = 3, FUN = median)

p_partial <- shp_italy |>
  mutate(SMR = SMR_partial_pooling) |>
  ggplot(aes(fill = SMR)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()

p_partial
```

## Comparing the models

Look at how each of the models alters the fit.

```{r}
p_full + labs(caption = "Full pooling model")
p_no + labs(caption = "No pooling model")
p_partial + labs(caption = "Partial pooling model")
```

There is clearly more smoothing in the partial pooling model.

## Closing remarks

In this lab session, we have XX
We looked at XX
