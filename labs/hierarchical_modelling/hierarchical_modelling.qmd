---
title: "Hierarchical modelling"
subtitle: "SHARP Bayesian Modeling for Environmental Health Workshop"
author: "Theo Rashid, Elizaveta Semenova"
date: "August 2023"
format: html
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(nimble)
library(posterior)
library(bayesplot)
library(sf)
library(hrbrthemes)
library(colorspace)

extrafont::loadfonts()
theme_set(theme_ipsum())

color_scheme_set(scheme = "viridis")

set.seed(2)
```

## Goal of this computing lab session

The goals of this lab is to introduce hierarchical modelling using the `NIMBLE` modelling framework.

During this lab session, we will:

1. Write a hierarchical model in `NIMBLE; and
2. Compare the model to more basic models, and discuss the advantages of hierarchical models.

In this task, we will explore mortality data in Italy using hierarchical modelling.
We will estimate the standardised mortality ratio (SMR) for populations in different provinces of Italy using three types of models:

1. Full pooling
2. No pooling
3. Partial pooling

::: aside
This workflow (fitting full/no/partial pooling models) is adapted from a classic Bayesian modelling example: the radon model. You can read about this model in Gelman and Hill's _Data Analysis Using Regression and Multilevel/Hierarchical Models_ (2006) or on the [tensorflow probability website](https://www.tensorflow.org/probability/examples/Multilevel_Modeling_Primer).
:::

## Exploratory data analysis

Let's load in the data.
```{r}
data <- read_rds(here("data", "italy", "italy_mortality.rds"))
glimpse(data)
summary(data)
```

Let's collapse the time dimension for now and focus on estimating the death rate in the final year and the 9th month.
```{r}
data <- data |>
  # make dummies for each province, useful for modelling
  mutate(provincia_id = data |> group_by(SIGLA) |> group_indices()) |>
  filter(year == 2018) |>
  filter(month == 9) |>
  arrange(SIGLA)
```

Let's look at the distribution the number of deaths in each province.
```{r}
#| label: fig-deaths-province
#| warning: false
#| fig-height: 10
#| fig-cap: Number of deaths at each time point in each province.

data |>
  ggplot(aes(x = SIGLA, y = deaths)) +
  geom_jitter(size = 0.4, alpha = 0.8, colour = "darkcyan") +
  coord_flip() +
  theme(axis.text.y = element_text(size = 7))
```

Let's plot the mean number of deaths in that month in that province.
```{r}
shp_italy <- read_rds(here("data", "italy", "italy_shp.rds")) |> arrange(SIGLA)

shp_italy |>
  left_join(
    data |>
      group_by(SIGLA) |>
      summarise(mean_deaths = mean(deaths))
  ) |>
  ggplot(aes(fill = mean_deaths)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()
```

We have a variable `expected`, which gives a good (if a slight underestimation) approximation to the number of deaths.
```{r}
data |>
  ggplot(aes(x = expected, y = deaths)) +
  geom_abline(slope = 1, intercept = 0, linewidth = 0.1) +
  geom_point(size = 0.3)
```

```{r}
data |>
  ggplot(aes(x = deaths / expected)) +
  geom_histogram()
```

We are going to estimate the region-specific SMR.
This is the ratio of actual deaths to expected deaths in each region.

## Full pooling model

First, let's treat all regions the same and estimate a single national prevalence.

The model is as follows:

Priors
$$
\alpha \sim N(0, 5)
$$

Likelihood
$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \log(E_i) + \alpha
\end{split}
$$

The parameter $\alpha$ here is common for all observations $y_i$, regardless of what province the observation belongs to.

```{r}
constants <- list(
  N = nrow(data),
  Np = max(data$provincia_id),
  province = data$provincia_id
)

inits <- list(list(alpha = 0), list(alpha = 0.1))

nimble_data <- list(
  y = as.integer(data$deaths),
  E = data$expected
)
```
```{r}
full_pooling_model <- nimbleCode({
  # priors
  alpha ~ dnorm(0, 5)

  # likelihood
  for (i in 1:N) {
    y[i] ~ dpois(mu[i])
    log(mu[i]) <- log(E[i]) + alpha
  }
})
```

```{r}
full_pooling_samples <- nimbleMCMC(
  code = full_pooling_model,
  data = nimble_data,
  constants = constants,
  inits = inits,
  monitors = c("alpha"),
  niter = 2000,
  nburnin = 1000,
  thin = 1,
  nchains = 2,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE,
  progressBar = TRUE
)
```

Follow the Bayesian workflow: look at MCMC outputs, check convergence using r-hat.

```{r}
summarise_draws(full_pooling_samples, default_summary_measures())
summarise_draws(full_pooling_samples, default_convergence_measures())
```

Let's plot the SMR in each province.
```{r}
full_pooling_posterior <- as_draws_array(full_pooling_samples)

SMR_full_pooling <- exp(full_pooling_posterior[, , "alpha"]) |>
  apply(MARGIN = 3, FUN = median)

p_full <- shp_italy |>
  mutate(SMR = SMR_full_pooling) |>
  ggplot(aes(fill = SMR)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()

p_full
```

Pretty boring, eh?

## No pooling model

Here, we treat all regions separately and estimate the SMR in each region separately with fixed effects.

Priors
$$
\alpha_j \sim N(0, 1) \quad j = 1,..., N_p
$$

Likelihood
$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \log(E_i) + \alpha_{j[i]}
\end{split}
$$

There is now an $\alpha$ for each province, so the map will no longer be uniform.

```{r}
inits <- list(
  list(
    alpha = rep(0, constants$Np)
  ),
  list(
    alpha = rep(0.1, constants$Np)
  )
)
```
```{r}
no_pooling_model <- nimbleCode({
  # priors
  for (j in 1:Np) {
    alpha[j] ~ dnorm(0, sd = 1)
  }

  # likelihood
  for (i in 1:N) {
    y[i] ~ dpois(mu[i])
    log(mu[i]) <- log(E[i]) + alpha[province[i]]
  }
})
```

```{r}
no_pooling_samples <- nimbleMCMC(
  code = no_pooling_model,
  data = nimble_data,
  constants = constants,
  inits = inits,
  monitors = c("alpha"),
  niter = 2000,
  nburnin = 1000,
  thin = 1,
  nchains = 2,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE,
  progressBar = TRUE
)
```

```{r}
summarise_draws(no_pooling_samples, default_summary_measures())
summarise_draws(no_pooling_samples, default_convergence_measures())
```

The SMR in each province
```{r}
no_pooling_posterior <- as_draws_array(no_pooling_samples)

SMR_no_pooling <- exp(no_pooling_posterior[, , 1:107]) |>
  apply(MARGIN = 3, FUN = median)

p_no <- shp_italy |>
  mutate(SMR = SMR_no_pooling) |>
  ggplot(aes(fill = SMR)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()

p_no
```

## Partial pooling model

This is also known as a varying intercept model.
We fit a random effect for each province.
The size of the random effect is controlled by the hyperparameter $\sigma_p$.
There is now a hierarchy in the parameters between $\sigma_p$ and the effects for each province $\theta$ â€“ hence, "hierarchical modelling".

Priors
$$
\begin{split}
\alpha &\sim N(0,5), \\
\sigma_p &\sim N^+(1) \\
\theta_j &\sim N(0, \sigma^2_p) \quad j = 1,..., N_p
\end{split}
$$

Likelihood
$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \log(E_i) + \alpha + \theta_{j[i]}
\end{split}
$$

```{r}
inits <- list(
  list(
    alpha = 0,
    theta = rep(0, constants$Np),
    sigma_p = 1
  ),
  list(
    alpha = 0.1,
    theta = rep(0, constants$Np),
    sigma_p = 1
  )
)
```
```{r}
partial_pooling_model <- nimbleCode({
  # priors
  alpha ~ dnorm(0, 5)
  sigma_p ~ T(dnorm(0, 1), 0, Inf) # half-normal

  for (j in 1:Np) {
    theta[j] ~ dnorm(0, sd = sigma_p)
  }

  # likelihood
  for (i in 1:N) {
    y[i] ~ dpois(mu[i])
    log(mu[i]) <- log(E[i]) + alpha + theta[province[i]]
  }
})
```

```{r}
partial_pooling_samples <- nimbleMCMC(
  code = partial_pooling_model,
  data = nimble_data,
  constants = constants,
  inits = inits,
  monitors = c("alpha", "sigma_p", "theta"),
  niter = 2000,
  nburnin = 1000,
  thin = 1,
  nchains = 2,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE,
  progressBar = TRUE
)
```

```{r}
summarise_draws(partial_pooling_samples, default_summary_measures())
summarise_draws(partial_pooling_samples, default_convergence_measures())
```

With more complicated models, sometimes it's nice to see if the traceplots are working too.
```{r}
mcmc_trace(partial_pooling_samples, regex_pars = "alpha")
```

There is a little bit more manipulation required to estimate the SMR for the more complicated model.
```{r}
partial_pooling_posterior <- as_draws_array(partial_pooling_samples)

SMR_partial_pooling <- exp(
  sweep(
    partial_pooling_posterior[, , 3:109],
    partial_pooling_posterior[, , 1],
    MARGIN = c(1, 2),
    FUN = "+"
  )
) |>
  apply(MARGIN = 3, FUN = median)

p_partial <- shp_italy |>
  mutate(SMR = SMR_partial_pooling) |>
  ggplot(aes(fill = SMR)) +
  geom_sf(colour = "white") +
  scale_fill_continuous_sequential(palette = "Reds") +
  theme_void()

p_partial
```

## Comparing the models

Look at how each of the models alters the fit.

```{r}
p_full + labs(caption = "Full pooling model")
p_no + labs(caption = "No pooling model")
p_partial + labs(caption = "Partial pooling model")
```

There is clearly more smoothing in the partial pooling model.
