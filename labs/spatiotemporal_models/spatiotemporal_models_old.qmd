---
title: "Spatial and Spatio-temporal Modelling"
subtitle: "SHARP Bayesian Modeling for Environmental Health Workshop"
author: "Garyfallos Konstantinoudis"
date: "August 2023"
format: html
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(nimble)
library(sf)
library(rgeos)
library(patchwork)
library(posterior)
library(bayesplot)
library(spdep)
library(lubridate)
library(fastDummies)
library(INLA)
library(coda)

extrafont::loadfonts()
theme_set(hrbrthemes::theme_ipsum())
knitr::opts_chunk$set(fig.align = "center")

set.seed(2)
```

## Goal of this computing lab session

This goal of this lab is to use `R` software Statistical Package (www.r-project.org), as well as `NIMBLE` to carry out a disease mapping study.

## What's going to happen in this lab session?

During this lab session, we will:

1. Explore ways of visualizing spatial data;
2. Define the neighborhood matrix in R;
3. Fit and interpret the BYM model in NIMBLE; and
4. Perform spatial ecological regression

## Introduction

We will use the COVID-19 deaths during March-July 2020, in England, at the LTLA geographical level (317 areas), as taken from the published paper:

__Konstantinoudis G__, Padellini T, Bennett J, Davies B, Ezzati M, Blangiardo M. _Long-term exposure to air-pollution and COVID-19 mortality in England: a hierarchical spatial analysis_. medRxiv [Preprint]. 2020 Aug 11:2020.08.10.20171421. doi: 10.1101/2020.08.10.20171421. Update in: Environ Int. 2021 Jan;146:106316. PMID: 32817974; PMCID: PMC7430619.

For that analysis, we included 38,573 COVID-19 deaths up to June 30, 2020 at the Lower Layer Super Output Area level in England (n=32 844 small areas). We retrieved averaged NO$_2$ concentration during 2014-2018 from the Pollution Climate Mapping. We used Bayesian hierarchical models to quantify the effect of air-pollution while adjusting for a series of confounding and spatial autocorrelation.

We will build simple Bayesian models to try to understand what is happening in the data. Once again we will use `NIMBLE` as the basis for our Bayesian model writing.

## Visualization of spatial areal data

Let's load in the data
```{r}
data_england <- read_sf(here("data", "England", "COVIDecoregression.shp"))
glimpse(data_england)
summary(data_england)
class(data_england)
```

The shapefile could be also imported using the function `readOGR` of the package `rgdal` (Geospatial Data Abstraction Library), such as `data_england <- readOGR(dsn = mypath, layer = "data_england")`, where the `dsn` argument specifies the data source name and `mypath` is the path where the file is stored. Then to convert `sp` object to `sf` object, we can use `st_as_sf` function, e.g., `data_england <- st_as_sf(data_england)`. This is not encouraged as the `rgdal` will be deprecated.


<!-- We can create basic maps of sf objects using the `plot()` function. The default plot of an sf object is a multi-plot of all attributes. -->

<!-- First plot all the attributes -->
<!-- ```{r eval=FALSE} -->
<!-- plot(data_england) -->
<!-- ``` -->

<!-- Next plot only the boundaries -->
<!-- ```{r eval=FALSE} -->
<!-- plot(data_england$geometry)  -->
<!-- ``` -->

We can make nice plots using the package `ggplot2`, which is very flexible and has lots of options for customising the way we want a plot to look.

First let's reduce the resolution to get quicker maps. Let's look at the full map of England and Wales for now, though we will reduce to just London later.
```{r fig.width=3, fig.align='center'}
data_england_simpler <- gSimplify(as(data_england, "Spatial"), tol = 500)
data_england_simpler <- st_as_sf(data_england_simpler)
data_england_simpler <- cbind(data_england_simpler, data_england %>% mutate(geometry = NULL))

ggplot() +
  geom_sf(data = data_england_simpler, color = "red", fill = "white") +
  ggtitle("Map of LTLAs in England") +
  coord_sf() + # axis limits and CRS
  labs(x = "Longitude", y = "Latitude", fill = "") +
  theme_bw() + # dark-on-light theme
  theme(
    axis.title = element_text(size = 16),
    axis.text = element_text(size = 14)
  )
```

And for nicer maps for the other columns, including COVID-19 deaths, the expected number of deaths, and the SMR:
```{r fig.width=11}
ggplot() +
  geom_sf(data = data_england_simpler, aes(fill = deaths)) +
  ggtitle("COVID-19 deaths") +
  theme_bw() +
  scale_fill_viridis_c(option = "A") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = expectd)) +
    ggtitle("Expected number of deaths") +
    theme_bw() +
    scale_fill_viridis_c(option = "D") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = deaths / expectd)) +
    ggtitle("Standardised mortality ratio") +
    theme_bw() +
    scale_fill_viridis_c(option = "B", name = "SMR")
```

And the covariates, including total ICU beds, NO$_2$ and IMD:
```{r fig.width=11}
ggplot() +
  geom_sf(data = data_england_simpler, aes(fill = TtlICUB)) +
  ggtitle("Total ICU beds") +
  theme_bw() +
  scale_fill_viridis_c(option = "A") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = NO2)) +
    ggtitle("NO2") +
    theme_bw() +
    scale_fill_viridis_c(option = "D") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = IMD)) +
    ggtitle("IMD") +
    theme_bw() +
    scale_fill_viridis_c(option = "B")
```


## A model with an unstructed spatial component

The relative risks (RRs) will be smoothed using the Poisson model with a log-link function, as we have seen in previous lectures and labs. As usual in this workshop, the inference is done with `NIMBLE` called through `R`.

In particular, let each area $i$ be indexed by  the integers $1, 2,...,N$. The model is as follows:

$$
\begin{eqnarray}
O_{i}  & \sim & \text{Pois}(\lambda_{i}E_{i} ) \quad i = 1, 2,...,N \\
\log(\lambda_{i}) & = & \alpha + \theta_{i}  \\
\theta_{i} & \sim & N(0,\sigma_{\theta})
\end{eqnarray}
$$

where $\sigma_{\theta}$ is a standard deviation term that controls the magnitude of $\theta_{i}$.

As in previous labs, we will write the model in `NIMBLE.` Specify the prior of $\tau_{\theta}$ in the code below (`tau.theta`). We can try a Gamma with parameters 1 and 0.01, which is a sensible option for a Poisson count model as it goes from 0 upwards, so can not be negative, like counts:
```{r}
UnstrCode <- nimbleCode({
  # priors
  alpha ~ dnorm(0, sd = 100000) # vague prior (large variance)
  tau.theta ~ dgamma(1, 0.01) # prior for the precision hyperparameter

  # likelihood
  for (i in 1:N) {
    O[i] ~ dpois(mu[i]) # Poisson likelihood
    log(mu[i]) <- log(E[i]) + alpha + theta[i]

    theta[i] ~ dnorm(0, tau = tau.theta) # area-specific RE
    RR[i] <- exp(alpha + theta[i]) # area-specific RR
  }

  # overall RR across study region
  overallRR <- exp(alpha)
})
```


The following code subsets the data to London so the models are quicker to run. The rest of the practical uses England as a whole:
```{r eval=FALSE, warning = FALSE}
data_england_simpler <- data_england[startsWith(data_england$LTLA, "E09"), ]
ggplot() +
  geom_sf(data = data_england_simpler, fill = "NA")
```

How many spatial units in the map?
```{r}
n.LTLA <- nrow(data_england_simpler)
n.LTLA
```

Create data object as required for `NIMBLE`.
```{r}
# Obtain the number of LTLAs
n.LTLA <- dim(data_england_simpler)[1]

# Format the data for NIMBLE in a list
COVIDdata <- list(
  O = data_england_simpler$deaths # observed nb of deaths
)

COVIDConsts <- list(
  N = n.LTLA, # number of LTLAs
  E = data_england_simpler$expectd # expected number of deaths
)
```


What are the parameters to be initialised? Create a list with two elements and call it `inits` (each a list) with different initial values for the parameters:
```{r eval=TRUE, echo=FALSE}
# Initialise the unknown parameters, 2 chains
inits <- list(
  list(alpha = 0.01, tau.theta = 10, theta = rep(0.01, times = n.LTLA)), # chain 1
  list(alpha = 0.5, tau.theta = 1, theta = rep(-0.01, times = n.LTLA)) # chain 2
)
```


Set `parameters_to_monitor` a vector to monitor `alpha`, `theta`, `tau.theta`, `overallRR`, `resRR`, `e` and `mu`.
```{r}
parameters_to_monitor <- c("alpha", "theta", "tau.theta", "overallRR", "RR", "mu")
```
Note that the parameters that are not set, will NOT be monitored!


Specify the MCMC setting, including number of samples, thining interval, burn-in period, and number of chains.
```{r}
ni <- 50000 # nb iterations
nt <- 100 # thinning interval
nb <- 30000 # nb iterations as burn-in
nc <- 2 # nb chains
```

The burn-in should be long enough to discard the initial part of the Markov chains that have not yet converged to the stationary distribution.

Run the MCMC simulations calling Nimble from R using the function `nimbleMCMC()`
```{r eval=FALSE, message=FALSE, warning=FALSE}
tic <- Sys.time()
modelGS.sim <- nimbleMCMC(
  code = UnstrCode,
  data = COVIDdata,
  constants = COVIDConsts,
  inits = inits,
  monitors = parameters_to_monitor,
  niter = ni,
  nburnin = nb,
  thin = nt,
  nchains = nc,
  setSeed = 9,
  progressBar = FALSE,
  samplesAsCodaMCMC = TRUE,
  summary = TRUE,
  WAIC = TRUE
)

toc <- Sys.time()
toc - tic # ~ 2minutes
saveRDS(modelGS.sim, file = "NIMBLE_IDD_A1")
```

```{r echo = FALSE}
modelGS.sim <- readRDS("NIMBLE_IDD_A1")
```

Note that specifying `samplesAsCodaMCMC = FALSE`, the function `nimbleMCMC()` returns a list object (if `codaPkg = TRUE`, file names of `NIMBLE` output are returned for easy access by the coda package).

What is the summary of each estimated parameter?
```{r}
summarise_draws(modelGS.sim$samples, default_summary_measures())
```

And how good do convergence indicators look?
```{r}
summarise_draws(modelGS.sim$samples, default_convergence_measures())
```

*The Gelman-Rubin diagnostic (Rhat)*

The Gelman-Rubin diagnostic evaluates MCMC convergence by analyzing the difference between multiple Markov chains. The convergence is assessed by comparing the estimated between-chains and within-chain variances for each model parameter. Large differences between these variances indicate nonconvergence.

When the scale reduction factor is high (perhaps greater than 1.1), then we should run our chains out longer to improve convergence to the stationary distribution.

You can use the function `rhat()` in the package `posterior` to calculate the *Rhat* and combine it with the function `mcmc_rhat()` in the `bayesplot` package to get a graphical illustration of the *Rhats*. First you need to create a list of different matrices per parameter. The different columns of these matrices are the different chains:
```{r eval=TRUE, warning=FALSE}
# extract the parameters you want to estimate rhat for
params.rhat <- modelGS.sim$samples$chain1 %>% colnames()
params.rhat %>% head()

# create the list of matrices containing the different chains
lapply(params.rhat, function(Y) do.call(cbind, modelGS.sim$samples[, Y])) -> list.mat
list.mat[[1]] %>% head()

# calculate rhat
sapply(list.mat, posterior::rhat) -> rhat_values
rhat_values %>% head()

# plot the rhat
bayesplot::mcmc_rhat(rhat_values) + theme_bw() + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

The *Rhat* of the overallRR is always lower than 1.1.


With more complicated models, sometimes it's nice to see if the traceplots are working too. Remember there are two chains of samples this time, which should look like they broadly have the same distribution if they have converged.
```{r eval = TRUE, warning = FALSE}
mcmc_trace(modelGS.sim$samples, pars = c("tau.theta"))
```

We can use the functions `mcmc_acf_bar()` and `mcmc_acf()` to get the autocorrelation plots for the different chains for `tau.theta` (which should be as near to `0` as possible).
```{r warning=FALSE}
mcmc_acf_bar(modelGS.sim$samples, pars = c("tau.theta"))
mcmc_acf(modelGS.sim$samples, regex_pars = c("tau.theta"))
```

To see the WAIC, which can be used to compare to other models to evaluate how well parameterised a model was.
```{r eval=TRUE}
modelGS.sim$WAIC
```

## Map of the (globally) smoothed RRs

Let's map the smoothed RRs in R we extract the posterior median of the relative risks
```{r eval=TRUE}
RR <- modelGS.sim$summary$all.chains[paste0("RR[", 1:n.LTLA, "]"), "Median"] # posterior median
```

Add it on the shapefile
```{r eval=TRUE}
data_england_simpler$RR <- RR
```

Using `ggplot2`, we can produce a map of the smoothed RRs
```{r eval=TRUE, fig.width=5, fig.align='center'}
ggplot() +
  geom_sf(data = data_england_simpler, aes(fill = RR)) +
  theme_bw() +
  scale_fill_viridis_c(limits = c(0, 2), name = "SMR")
```


## The BYM model

The RRs will be smoothed using the Poisson model and the BYM model in the log-link function. The inference is done with `NIMBLE` called through R. In particular, let each area $i$ be indexed by  the integers $1, 2,...,N$. The model is as follows.

::: aside
DESCRIPTION OF BYM
:::

$$
\begin{equation}
\begin{aligned}
\hbox{O}_i & \sim \hbox{Poisson}(E_i \lambda_i); \;\;\; i=1,...,N\\
\log \lambda_i & = \alpha + \theta_i + \phi_i\\
\theta_i &\sim \hbox{Normal}(0, \sigma^2_{\theta_i})\\
{\bf \phi} & \sim \hbox{ICAR}({\bf W}, \sigma_{\phi}^2) \,\, ,  \sum_i \phi_i  = 0 \\
\alpha & \sim \text{Uniform}(-\infty, +\infty) \\
1/\sigma_{\theta}^2 & \sim \hbox{Gamma}(0.5, 0.05) \\
1/\sigma_{\phi}^2 & \sim \hbox{Gamma}(0.5, 0.0005) \\
\end{aligned}
\end{equation}
$$

where $\sigma_{\phi}$ is a standard deviation term that controls the magnitude of $\phi_{i}$, i.e. the spatial structured term.

Run the MCMC simulations calling `NIMBLE` from R using the function `nimbleMCMC()`
```{r}
BYMCode <- nimbleCode({
  # priors
  alpha ~ dflat() # vague prior (Unif(-inf, +inf))
  overallRR <- exp(alpha) # overall RR across study region

  tau.theta ~ dgamma(0.5, 0.05) # prior for the precision hyperparameter
  sigma2.theta <- 1 / tau.theta # variance of unstructured area random effects

  tau.phi ~ dgamma(0.5, 0.0005) # prior on precision of spatial area random effects
  sigma2.phi <- 1 / tau.phi # conditional variance of spatial area random effects

  # likelihood
  for (i in 1:N) {
    O[i] ~ dpois(mu[i]) # Poisson likelihood for observed counts
    log(mu[i]) <- log(E[i]) + alpha + theta[i] + phi[i]

    theta[i] ~ dnorm(0, tau = tau.theta) # area-specific RE

    SMR[i] <- exp(alpha + theta[i] + phi[i])
    resRR[i] <- exp(theta[i] + phi[i]) # area-specific residual RR
    proba.resRR[i] <- step(resRR[i] - 1) # Posterior probability
  }

  # BYM
  phi[1:N] ~ dcar_normal(adj = adj[1:L], weights = weights[1:L], num = num[1:N], tau = tau.phi, zero_mean = 1)
})
```

# Creating the adjacency matrix

To fit a BYM model, we first need to define an adjacency matrix, which describes which spatial units are actually neighbouring each other. The equation is below, but essentially if the units neighbour, they are given `1` in the relevant adjacency matrix, otherwise the element representing whether two spatial units are neighbouring (sometimes called 'contiguous') then it is given a value of `0`. Most of the elements in a matrix will be `0` for a real-world situation with lots of spatial units.

To run the BYM model, the adjacency matrix needs to be provided. Recall that there are many ways of defining an adjacency matrix ${\bf W}$. Here we will use queen contiguity which is defined as
$$
\begin{equation}
w_{ij} =
\begin{cases}
1 & \text{if } j \in \partial_i  \\
0         & \text{otherwise}
\end{cases}
\end{equation}
$$

where $\partial_i$ is the set of area adjacent to $i$, and $w_{ij}$ is the $ij$ element of ${\bf W}$.

# Adjacency matrix in R


Convert the polygons to a list of neighbors using the function `poly2nb()`.
```{r warning = FALSE}
LTLA_nb <- poly2nb(pl = data_england_simpler)
LTLA_nb
# extract centroids from the England shp
centr <- st_centroid(data_england_simpler) %>% st_geometry()
```

As you can see from the plot below, the matrix will represent the network of connections as represented by the black graph below
```{r}
par(mar = c(0, 0, 0, 0))
plot(data_england_simpler$geometry, border = "grey66")
plot(LTLA_nb, centr, pch = 19, cex = .5, add = T)
```

Convert the list you defined previously to NIMLE format (i.e. a list of 3 components adj, num and weights) using the function `nb2WB()` and print a summary of the object.
```{r}
nbWB_A <- nb2WB(nb = LTLA_nb)
names(nbWB_A)
```

A list of three components is created

1. `adj` = `ID` for all the neighbors;
2. `weights` = the weight for each neighbour; and
3. num = total nb of neighbors across the study region

Create data and constants objects as required for `NIMBLE`.
```{r eval=TRUE}
n.LTLA <- dim(data_england_simpler)[1]

# Format the data for NIMBLE in a list
COVIDdata <- list(
  O = data_england_simpler$deaths # observed nb of deaths
)

COVIDConsts <- list(
  N = n.LTLA, # nb of LTLAs

  # adjacency matrix
  L = length(nbWB_A$weights), # the number of neighboring areas
  E = data_england_simpler$expectd, # expected number of deaths
  adj = nbWB_A$adj, # the elements of the neighbouring matrix
  num = nbWB_A$num,
  weights = nbWB_A$weights
)
```


Define the initial values for ALL the unknown parameters:
```{r echo=TRUE, eval=TRUE}
# initialise the unknown parameters, 2 chains
inits <- list(
  list(
    alpha = 0.01,
    tau.theta = 10,
    tau.phi = 1,
    theta = rep(0.01, times = n.LTLA),
    phi = c(rep(0.5, times = n.LTLA))
  ),
  list(
    alpha = 0.5,
    tau.theta = 1,
    tau.phi = 0.1,
    theta = rep(0.05, times = n.LTLA),
    phi = c(rep(-0.05, times = n.LTLA))
  )
)
```

Which model parameters do you want to monitor? Set these before running `NIMBLE` Call this object `parameters_to_monitor`.
```{r echo=TRUE, eval=TRUE}
parameters_to_monitor <- c("sigma2.theta", "sigma2.phi", "overallRR", "theta", "SMR", "resRR", "proba.resRR", "alpha")
```

Specify the MCMC setting, as we always do
```{r echo=TRUE, eval=TRUE}
ni <- 50000 # nb iterations
nt <- 10 # thinning interval
nb <- 10000 # nb iterations as burning
nc <- 2 # nb chains
```


Let's run the model, this needs approximately 2 minutes.
```{r eval = FALSE}
tic <- Sys.time()
modelBYM.sim <- nimbleMCMC(
  code = BYMCode,
  data = COVIDdata,
  constants = COVIDConsts,
  inits = inits,
  monitors = parameters_to_monitor,
  niter = ni,
  nburnin = nb,
  thin = nt,
  nchains = nc,
  setSeed = 9,
  progressBar = FALSE,
  samplesAsCodaMCMC = TRUE,
  summary = TRUE,
  WAIC = TRUE
)
toc <- Sys.time()
toc - tic
saveRDS(modelBYM.sim, file = "NIMBLE_BYM_A3")
```

```{r echo=TRUE, eval=TRUE}
modelBYM.sim <- readRDS("NIMBLE_BYM_A3")
```

Retrieve WAIC and compare with previous model. Which model performs best?
```{r echo=TRUE, eval=TRUE}
modelBYM.sim$WAIC
```

Check convergence of resRR
```{r echo=TRUE, eval=TRUE, warning = FALSE}
mcmc_trace(modelBYM.sim$samples, pars = c("resRR[1]", "resRR[5]", "resRR[15]", "resRR[19]"))
```

(a) Extract the residual RR (i.e. exp(V + U)) and the posterior probability that resRR is higher than 1 as. Create a `data.frame` for this purpose. Note that for the posterior probability, which is a vector of 0s and 1s we need to calculate the sum of 1s by the total sum, ie the mean.
```{r echo=TRUE, eval=TRUE}
RR_BYM <- data.frame(
  RR_BYM = modelBYM.sim$summary$all.chains[paste0("resRR[", 1:n.LTLA, "]"), "Median"],
  pp_BYM = modelBYM.sim$summary$all.chains[paste0("proba.resRR[", 1:n.LTLA, "]"), "Mean"]
)
```

(b) Add a column ID for each LTLA, from 1 to `r n.LTLA`, to *RR_BYM*.
```{r echo=TRUE, eval=TRUE}
RR_BYM$LTLA <- data_england_simpler$LTLA
```

(c) Change the name of the column corresponding to the posterior mean to BYM
```{r echo=TRUE, eval=TRUE}
colnames(RR_BYM) <- c("BYM", "pp_resRR", "LTLA")
```


(d) Merge RR_BYM with COVID19Deaths using LTLA as column for merging. Call the new object *COVID19Deaths*.
```{r echo=TRUE, eval=TRUE}
COVID19Deaths <- left_join(data_england_simpler, RR_BYM, by = c("LTLA" = "LTLA"))
```


(e) Map the smoothed residual RR (resRR). You can use the `ggplot` function for this purpose:
```{r echo=TRUE, eval=TRUE, fig.width=10, warning=FALSE}
ggplot() +
  geom_sf(data = COVID19Deaths, aes(fill = BYM)) + # standard map
  scale_fill_viridis_c(name = "RR", limits = c(0, 2)) +
  ggtitle("Posterior median RR") +
  theme(
    axis.title.x = element_blank(), # removes xaxis title
    axis.title.y = element_blank() # removes yaxis title
  ) |
  ggplot() +
    geom_sf(data = COVID19Deaths, aes(fill = pp_resRR)) +
    scale_fill_viridis_c(name = "Prob") +
    ggtitle("Exceedance probability") +
    theme(
      axis.title.x = element_blank(), # removes xaxis title
      axis.title.y = element_blank() # removes yaxis title
    )
```


## Closing remarks

In this lab session, we have explored how to fit spatial models using Bayesian regression in `NIMBLE`.
We looked at the two most common approaches: iid and the BYM model.

We used real data on COVID-19 deaths in England during March-July 2020 and first performed disease mapping to understand the spatial trends of COVID-19 mortality in the first stages of the pandemic. As part of this analysis, we visualised spatial data, fitted a model with an unstructured spatial random effect, demonstrated how to build a neihgborhood matrix in R and fitted the BYM model.

For more information on the final model see Konstantinoudis et al 2021 (10.1016/j.envint.2020.106316).

## Advanced: Ecological regression with the BYM model

The section below is very relevant for environmental health, but if we run out of time in the lab, it is something you should go through in your own time.

Let $\mathcal{D}$ be the observation window of England and $A_1, A_2, \dots, A_N$ a partition denoting the LTLAs in England with $\cup_{i=1}^NA_i = \mathcal{D}$ and $A_i\cap A_j$ for every $i\neq j$. Let $O_1, O_2, \dots, O_N$ be the observed number of COVID-19 deaths occurred during March-July 2020 in England, $E_1, E_2, \dots, E_N$ is the expected number of COVID-19 deaths and $\lambda_1, \lambda_2, \dots, \lambda_N$ the standardized mortality ratio (recall $\lambda_i = \frac{O_i}{E_i}$). A standardized mortality ratio of $1.5$ implies that the COVID-19 deaths we observed in the $i$-th area are $1.5$ times higher to what we expected. Under the Poisson assumption we have:
$$
\begin{equation}
\begin{aligned}
\hbox{O}_i & \sim \hbox{Poisson}(E_i \lambda_i); \;\;\; i=1,...,N\\
\log \lambda_i & = \alpha +  \beta_1 X_{1i} + \beta_2 X_{2i} + \theta_i + \phi_i\\
\theta_i &\sim \hbox{Normal}(0, \sigma^2_{\theta_i})\\
{\bf \phi} & \sim \hbox{ICAR}({\bf W}, \sigma_{\phi}^2) \,\, ,  \sum_i \phi_i  = 0 \\
\alpha & \sim \text{Uniform}(-\infty, +\infty) \\
\beta_1, \beta_2 & \sim \mathcal{N}(0, 10) \\
1/\sigma_{\theta}^2 & \sim \hbox{Gamma}(0.5, 0.05) \\
1/\sigma_{\phi}^2 & \sim \hbox{Gamma}(0.5, 0.0005) \\
\end{aligned}
\end{equation}
$$

the terms $\beta_1 X_{1i} + \beta_2 X_{2i} + \sum_{j=2}^5\beta_{3j} X_{3i}$, where $X_{1i}, X_{2i}, X_{3i}$ are the ICU beds, NO$_2$ and IMD in the $i$-th LTLA, $\beta_1, \beta_2, \sum_{j=2}^5\beta_{3j}$ the corresponding effects and $exp(\beta_1), exp(\beta_2)$ the relative risk of ICU beds or NO$_2$ for every unit increase and of the ICU beds or NO$_2$. For instance $exp(\beta_2) = 1.8$ means that for every unit increase of long term exposure to $NO_2$, the risk (read standardized mortality ratio) of COVID-19 deaths cancer increases by $80\%$. $exp(\beta_{32}), \beta_{33}, \beta_{34}, \beta_{35}$ are the relative risks compared to the baseline IMD category, ie the most deprived areas. An $exp(\beta_{35}) = 0.5$ means that the risk of COVID-19 deaths in most affluent areas decreases by $50%$ compared to the most deprived areas.$\tau_{\theta}$ is a precision (reciprocal of the variance) term that controls the magnitude of $\theta_{i}$. We will first write the model in `NIMBLE`.

```{r eval=TRUE, echo = TRUE}
BYMecoCode <- nimbleCode({
  for (i in 1:N) {
    O[i] ~ dpois(mu[i]) # Poisson likelihood for observed counts
    log(mu[i]) <- log(E[i]) + alpha + theta[i] + phi[i] + inprod(beta[], X[i, ])
    # the inprod is equivalent with beta[1]*X1[i] + beta[2]*X2[i] + beta[3]*X32[i] + beta[4]*X33[i] + beta[5]*X34[i] + beta[6]*X35[i]

    SMR[i] <- alpha + theta[i] + phi[i] + inprod(beta[], X[i, ])
    theta[i] ~ dnorm(0, tau = tau.theta) # area-specific RE
    resRR[i] <- exp(theta[i] + phi[i]) # area-specific residual RR
    proba.resRR[i] <- step(resRR[i] - 1) # Posterior probability
  }

  # BYM prior
  phi[1:N] ~ dcar_normal(adj = adj[1:L], weights = weights[1:L], num = num[1:N], tau = tau.phi, zero_mean = 1)

  # Priors
  alpha ~ dflat() # vague prior (Unif(-inf, +inf))
  overallRR <- exp(alpha) # overall RR across study region

  tau.theta ~ dgamma(0.5, 0.05) # prior for the precision hyperparameter
  sigma2.theta <- 1 / tau.theta # variance of unstructured area random effects

  tau.phi ~ dgamma(0.5, 0.0005) # prior on precison of spatial area random effects
  sigma2.phi <- 1 / tau.phi # conditional variance of spatial area random effects

  # priors for the fixed effects
  for (j in 1:K) {
    beta[j] ~ dnorm(0, tau = 1)
    RR.beta[j] <- exp(beta[j])
  }

  RR.beta1_1NO2 <- exp(beta[1] / sd.no2) # get per 1 unit increase in the airpollution (scale back)
})
```


Create data object as required for `NIMBLE`:
```{r eval=TRUE, warning=FALSE}
n.LTLA <- dim(data_england_simpler)[1]

# create the dummies for deprivation:
fastDummies::dummy_cols(data_england_simpler$IMD) %>%
  as_tibble() %>%
  select(.data_1:.data_5) %>%
  rename(
    IMD_1 = .data_1, IMD_2 = .data_2, IMD_3 = .data_3,
    IMD_4 = .data_4, IMD_5 = .data_5
  ) -> dummies
data_england_simpler <- cbind(data_england_simpler, dummies)

# Matrix of covariates
Xmat <- cbind(
  scale(data_england_simpler$NO2)[, 1],
  scale(data_england_simpler$TtlICUB)[, 1],
  data_england_simpler$IMD_2,
  data_england_simpler$IMD_3,
  data_england_simpler$IMD_4,
  data_england_simpler$IMD_5
)

# number of total covariates
K <- ncol(Xmat)

# Format the data for NIMBLE in a list
COVIDdata <- list(
  O = data_england_simpler$deaths, # observed nb of deaths

  # covariates
  X = Xmat
)

COVIDConsts <- list(
  N = n.LTLA, # nb of LTLAs
  K = K,
  # adjacency matrix
  L = length(nbWB_A$weights), # the number of neighboring areas
  E = data_england_simpler$expectd, # expected number of deaths
  adj = nbWB_A$adj, # the elements of the neighboring matrix
  num = nbWB_A$num,
  weights = nbWB_A$weights,
  sd.no2 = data_england_simpler$NO2 %>% sd()
)
```

Create the initial values for ALL the unknown parameters:
```{r echo=TRUE, eval=TRUE}
# initialise the unknown parameters, 2 chains
inits <- list(
  list(
    alpha = 0.01,
    beta = rep(0, K),
    tau.theta = 10,
    tau.phi = 1,
    theta = rep(0.01, times = n.LTLA),
    phi = c(rep(0.5, times = n.LTLA))
  ),
  list(
    alpha = 0.5,
    beta = rep(-1, K),
    tau.theta = 1,
    tau.phi = 0.1,
    theta = rep(0.05, times = n.LTLA),
    phi = c(rep(-0.05, times = n.LTLA))
  )
)
```


Which model parameters do you want to monitor? Set these before running `NIMBLE` Call this object `parameters_to_monitor`.
```{r echo=TRUE, eval=TRUE}
parameters_to_monitor <- c("sigma2.theta", "sigma2.phi", "overallRR", "theta", "beta", "RR.beta", "resRR", "proba.resRR", "alpha", "RR.beta1_1NO2")
```

Specify the MCMC setting
```{r echo=TRUE, eval=TRUE}
ni <- 50000 # nb iterations
nt <- 10 # thinning interval
nb <- 10000 # nb iterations as burning
nc <- 2 # nb chains
```

Run the MCMC simulations calling `NIMBLE` from R using the function `nimbleMCMC()`. If everything is specified reasonably, this needs approximately 5 minutes.
```{r echo=TRUE, eval=FALSE}
tic <- Sys.time()
modelBYMeco.sim <- nimbleMCMC(
  code = BYMecoCode,
  data = COVIDdata,
  constants = COVIDConsts,
  inits = inits,
  monitors = parameters_to_monitor,
  niter = ni,
  nburnin = nb,
  thin = nt,
  nchains = nc,
  setSeed = 9,
  progressBar = FALSE,
  samplesAsCodaMCMC = TRUE,
  summary = TRUE,
  WAIC = TRUE
)
toc <- Sys.time()
toc - tic
saveRDS(modelBYMeco.sim, file = "NIMBLE_BYM_A4")
```

```{r echo=TRUE, eval=TRUE}
modelBYMeco.sim <- readRDS("NIMBLE_BYM_A4")
```

Retrieve WAIC and compare with previous model. Which model performs best?
```{r echo=TRUE, eval=TRUE, warning=FALSE}
modelBYMeco.sim$WAIC
```


Check the convergence of the intercept and the different covariates. What do you observe?
```{r echo=TRUE, eval=TRUE, warning = FALSE, fig.height=8, fig.width=10}
mcmc_trace(modelBYMeco.sim$samples, pars = c("alpha", paste0("beta[", 1:K, "]")))
```

Retrieve summary statistics for the different covariates covariates and interpret (it is easier to interpret on the relative scale):
```{r echo=TRUE, eval=TRUE}
modelBYMeco.sim$summary$all.chains[paste0("RR.beta[", 1:K, "]"), ]
```

We can get a nice credible intervals plot as well:
```{r warning=FALSE}
modelBYMeco.sim$summary$all.chains[paste0("RR.beta[", 1:K, "]"), ] %>%
  as_tibble() %>%
  dplyr::select(Median, `95%CI_low`, `95%CI_upp`) %>%
  mutate(
    covariate =
      factor(c("NO2", "ICU", paste0("IMD", 2:5)),
        levels = c("NO2", "ICU", paste0("IMD", 2:5))
      )
  ) -> cov.eff

cov.eff %>% head()

cov.eff %>%
  ggplot() +
  geom_point(aes(x = covariate, y = Median)) +
  geom_errorbar(aes(x = covariate, ymin = `95%CI_low`, ymax = `95%CI_upp`), width = 0.2) +
  ylim(c(0.5, 1.5)) +
  geom_hline(yintercept = 1, lty = 2, col = "red")
```
The effect by unit increase in the long term exposure to NO$_2$ is `RR.beta1_1NO2`:
```{r}
# as relative risk per 1 unit increase in the long term NO2 exposure
modelBYMeco.sim$summary$all.chains[paste0("RR.beta1_1NO2"), c("Median", "95%CI_low", "95%CI_upp")]

# as percentage increase in mortality for 1 unit increase in the long term NO2 exposure
(modelBYMeco.sim$summary$all.chains[paste0("RR.beta1_1NO2"), c("Median", "95%CI_low", "95%CI_upp")] - 1) * 100
```
For every 1$\mu$g/$m^3$ increase in the long term exposure to NO$_2$ the COVID-19 mortality increase by 3\%.


Compare the BYM-related spatial field before and after adjusting for covariates.
```{r fig.width=10, warning=FALSE}
COVID19Deaths$BYM_ECO <- modelBYMeco.sim$summary$all.chains[paste0("resRR[", 1:n.LTLA, "]"), "Median"]

ggplot() +
  geom_sf(data = COVID19Deaths, aes(fill = BYM)) + # standard map
  scale_fill_viridis_c(name = "unadj", limits = c(0, 2.2)) +
  ggtitle("Posterior median RR (unadj)") +
  theme(
    axis.title.x = element_blank(), # removes xaxis title
    axis.title.y = element_blank() # removes yaxis title
  ) |
  ggplot() +
    geom_sf(data = COVID19Deaths, aes(fill = BYM_ECO)) +
    scale_fill_viridis_c(name = "adj", limits = c(0, 2.2)) +
    ggtitle("Posterior median RR (adj)") +
    theme(
      axis.title.x = element_blank(), # removes xaxis title
      axis.title.y = element_blank() # removes yaxis title
    )
```


## Further closing remarks for Advanced section

If you got through the advanced portion, we examined the effect of long term exposure to NO$_2$ on COVID-19 mortality. We fitted a BYM model to account for unknown spatial confounding but in addition we accounted for total number of ICU beds and deprivation per LTLA. We reported evidence of an increased COVID-19 mortality for increasing levels of NO$_2$.
