---
title: "Introduction to Bayesian Methods"
subtitle: "SHARP Bayesian Modeling for Environmental Health Workshop"
author: "Robbie M. Parks"
date: "August 2023"
output: html_document
---


## Load pacakges

```{r}
library(here)
library(tidyverse)
library(nimble)
library(bayesplot)
library(posterior)
```


## Goal of this computing lab session

This lab will involve taking some concepts from the lectures and introduce you to the way NIMBLE works.

## Introduction to NIMBLE format.

NIMBLE is written in a slightly unusual format if you're used to just using R. It is written in the style of a program called BUGS, which came out a few decades ago and was developed at Imperial College London. 

## First basic examples of NIMBLE and how to use it.

These examples will be for basic regression modesl using linear predictors

Adapted from https://r-nimble.org/examples

## Normal-Normal example.

The first example will utilize a simple regression

First create some example data for our model:

$$
y = max + c
$$

```{r}
set.seed(1)
p <- 3    # number of explanatory variables
n <- 100   # number of observations
X <- matrix(round(rnorm(p*n),2), nrow = n, ncol = p) # explanatory variables
true_betas <- c(c(0.2, 0.5, 0.3)) # coefficients
sigma <- 0.5
y <- rnorm(n, X %*% true_betas, sigma)
```


What does the dataset look like?

```{r}
head(data.frame(y=y,x1=X[,1],x2=X[,2],x3=X[,3]))
```


What does equivalent frequentist model output look like for reference? What's the interpretation of the 95% CI?

```{r}
model_freq <- lm(y~x1+x2+x3,data=data.frame(y=y,x1=X[,1],x2=X[,2],x3=X[,3]))
central_est <- t(t(model_freq$coefficients))
conf_int <- confint(model_freq)
print(cbind(central_est, conf_int))
```


Create the NIMBLE model

```{r}
code <- nimbleCode({
  
  # priors for parameters
  beta0 ~ dnorm(0, sd = 100) # prior for beta0
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100)      # prior for variance components
  
  # regression formula
  for(i in 1:n) {
    y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i], sd = sigma) # manual entry of linear predictors
  }
  
})
```


Before running NIMBLE, extract data for three predictors and center around zero for better MCMC performance

```{r}
x1 <- X[,1] - mean(X[,1])
x2 <- X[,2] - mean(X[,2])
x3 <- X[,3] - mean(X[,3])
```


Final preparation of data into lists

```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```


Set initial values for MCMC samples

```{r}
inits <- list(beta0 = mean(y), beta1 = 0, beta2 = 0, beta3 = 0, sigma = 1)
```


The following code will establish which samples will be used in the sampling of the posteriors. If there is a conjugate relationship apparent between prior and posterior (e.g., Normal-Normal, Binomial-Beta, Poisson-Gamma), it will be detected here

```{r}
model <- nimbleModel(code, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model)
```


Specify the number of MCMC samples

```{r}
ni = 10000
```


Run the MCMC simulations

```{r}
t0 = Sys.time()
nimbleMCMC_samples_initial = nimbleMCMC(
                           code = code,
                           data = data,
                           constants = constants, 
                           inits = inits,
                           niter = ni,
                           setSeed = 1,
                           summary = TRUE,
                           samplesAsCodaMCMC=TRUE,
                           WAIC= TRUE)
t1 =  Sys.time()
t1 - t0
```


What is the summary of each estimated parameter from the samples?

```{r}
nimbleMCMC_samples_initial$summary

# Below doesn't currently work
# posterior::summarise_draws(nimbleMCMC_samples_initial)
# bayesplot::mcmc_trace(nimbleMCMC_samples_initial)
```


What do the samples of one of the unknown parameters actually look like? Let's have a look at beta1 (which we know is 0.2)

```{r}
plot(nimbleMCMC_samples_initial$samples[ , 'beta1'], type = 'l', xlab = 'iteration',  
     ylab = expression(Beta_1))
```


So it looks like the samples are converging quickly from the initial parameter to ~0.2. But typically we will throw some samples at the beginning to ensure that the transient samples (which is when the model samples haven't stabilized around a particular value) are not included in calculating estimates of the mean and credible intervals. This is called the 'burn in' or 'warm up period'.

Let's do it again but with a burn in of 1000 samples.

```{r}
nb = 1000 

t0 = Sys.time()
nimbleMCMC_samples_burnin = nimbleMCMC(code = code,
                           data = data,
                           constants = constants, 
                           inits = inits,
                           niter = ni,
                           nburnin = nb,
                           setSeed = 1,
                           summary = TRUE,
)
t1 =  Sys.time()
t1 - t0
```


What is the summary of each estimated parameter from the samples with burn in?

```{r}
nimbleMCMC_samples_burnin$summary
```


Now the samples look to be very tidily centered around 0.2.

```{r}
plot(nimbleMCMC_samples_burnin$samples[ , 'beta1'], type = 'l', xlab = 'iteration',  
     ylab = expression(Beta_1))
```


What's the influence of tighter priors?

```{r}
code_tighter_priors = nimbleCode({
  
  # priors for parameters
  beta0 ~ dnorm(0, sd = 1) # prior for beta0
  beta1 ~ dnorm(0, sd = 1) # prior for beta1
  beta2 ~ dnorm(0, sd = 1) # prior for beta2
  beta3 ~ dnorm(0, sd = 1) # prior for beta3
  sigma ~ dunif(0, 1)      # prior for variance components
  
  # regression formula
  for(i in 1:n) {
    y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i], sd = sigma) # manual entry of linear predictors
  }
  
})
```


Run the MCMC simulations

```{r}
t0 = Sys.time()
nimbleMCMC_samples_tighter_priors = nimbleMCMC(
                           code = code_tighter_priors,
                           data = data,
                           constants = constants, 
                           inits = inits,
                           niter = ni,
                           nburnin = nb,
                           setSeed = 1,
                           summary = TRUE)
t1 =  Sys.time()
t1 - t0
```


What is the summary of each estimated parameter from the samples with tighter priors?

```{r}
nimbleMCMC_samples_tighter_priors$summary
```

Say we wanted to establish the estimated difference between two betas? Let's say Beta_1 and Beta_2 in this case

```{r}
code_diff_betas = nimbleCode({
  
  # priors for parameters
  beta0 ~ dnorm(0, sd = 100) # prior for beta0
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100)      # prior for variance components
  
  # regression formula
  for(i in 1:n) {
    y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i], sd = sigma) # manual entry of linear predictors
  }
  
  # difference between beta1 and beta2
  beta12_diff <- beta2 - beta1
  
})
```


Run the MCMC simulations for monitoring specifically differences between beta1 and beta2

```{r}
parameters_to_monitor = c('beta12_diff')

t0 = Sys.time()
nimbleMCMC_samples_diff_betas = nimbleMCMC(
                           code = code_diff_betas,
                           data = data,
                           constants = constants, 
                           inits = inits,
                           monitors = parameters_to_monitor,
                           niter = ni,
                           nburnin = nb,
                           setSeed = 1,
                           summary = TRUE)
t1 =  Sys.time()
t1 - t0
```


What is the summary of each estimated parameter from the samples when monitoring difference between beta1 and beta2?

```{r}
nimbleMCMC_samples_diff_betas$summary
```


## Logistic regression example.

First create some example data for our model:

$$
y = max + c
$$


```{r}
n = 10000
p = 3

set.seed(1)
x1 = round(rnorm(n),2)
x2 = round(rnorm(n),2)
x3 = round(rnorm(n),2)

z = 1 + 2*x1 + 3*x2 - 5*x3       # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = rbinom(n,1,pr)      # bernoulli response variable
```


What does the dataset look like?

```{r}
head(data.frame(y=y,x1=x1,x2=x2,x3=x3))
```


What does equivalent frequentist model output look like for reference? What's the interpretation of the 95% CI?

```{r}
#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2,x3=x3)
model_freq = glm(y~x1+x2+x3,data=df,family="binomial")
central_est = t(t(model_freq$coefficients))
conf_int = confint(model_freq)
print(cbind(central_est, conf_int))
```


Create the NIMBLE model

```{r}
code_binomial <- nimbleCode({
  
  # priors for parameters
  beta0 ~ dnorm(0, sd = 100) # prior for beta0
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  # shape1 ~ dunif(0, 100)     # prior for shape1 of beta
  # shape2 ~ dunif(0, 100)     # prior for shape1 of beta

  # regression formula
  for(i in 1:n) {
    y[i] ~ dbin(p[i], 1)
    logit(p[i]) <- beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
  }
  
})
```


Before running NIMBLE, extract data for three predictors and center around zero for better MCMC performance

```{r}
x1 <- x1 - mean(x1)
x2 <- x2 - mean(x2)
x3 <- x3 - mean(x3)
```


Final preparation of data into lists

```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```


Set initial values for MCMC samples

```{r}
inits <- list(beta0 = 0, beta1 = 0, beta2 = 0, beta3 = 0)
```


The following code will establish which samples will be used in the sampling of the posteriors. If there is a conjugate relationship apparent between prior and posterior (e.g., Normal-Normal, Binomial-Beta, Poisson-Gamma), it will be detected here

```{r}
model_binomial <- nimbleModel(code_binomial, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model_binomial)
```


Let's run the model.

```{r}
ni = 10000
nb = 1000 

t0 = Sys.time()
nimbleMCMC_samples_binomial = nimbleMCMC(code = code_binomial,
                           data = data,
                           constants = constants, 
                           inits = inits,
                           niter = ni,
                           nburnin = nb,
                           setSeed = 1,
                           summary = TRUE,
)
t1 =  Sys.time()
t1 - t0
```


What is the summary of each estimated parameter from the binomial model?

```{r}
nimbleMCMC_samples_binomial$summary
```


Now the samples look to be very tidily centered around 2.

```{r}
plot(nimbleMCMC_samples_binomial$samples[ , 'beta1'], type = 'l', xlab = 'iteration',  
     ylab = expression(Beta_1))
```


## Poisson-Gamma example.

First create some example data for our model:

$$
y = max + c
$$


```{r}
n = 10000

set.seed(1)
x1 = round(rnorm(n),2)
x2 = round(rnorm(n),2)
x3 = round(rnorm(n),2)

z = 4 + 0.3*x1 - 0.1*x2 + 0.6*x3       
lambda = exp(z)       
y = rpois(n,lambda)    
```


What does the dataset look like?

```{r}
head(data.frame(y=y,x1=x1,x2=x2,x3=x3))
```


What does equivalent frequentist model output look like for reference? What's the interpretation of the 95% CI?

```{r}
#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2,x3=x3)
model_freq = glm(y~x1+x2+x3,data=df,family="poisson")
central_est = t(t(model_freq$coefficients))
conf_int = confint(model_freq)
print(cbind(central_est, conf_int))
```


Create the NIMBLE model

```{r}
code_poisson <- nimbleCode({
  
  # priors for parameters
  beta0 ~ dnorm(0, sd = 100) # prior for beta0
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3

  # regression formula
  for(i in 1:n) {
    y[i] ~ dpois(lambda[i])
    log(lambda[i]) <- beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
  }
  
})
```


Before running NIMBLE, extract data for three predictors and center around zero for better MCMC performance

```{r}
x1 <- x1 - mean(x1)
x2 <- x2 - mean(x2)
x3 <- x3 - mean(x3)
```


Final preparation of data into lists

```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```


Set initial values for MCMC samples

```{r}
inits <- list(beta0 = 0, beta1 = 0, beta2 = 0, beta3 = 0)
```


The following code will establish which samples will be used in the sampling of the posteriors. If there is a conjugate relationship apparent between prior and posterior (e.g., Normal-Normal, Binomial-Beta, Poisson-Gamma), it will be detected here

```{r}
model_poisson <- nimbleModel(code_poisson, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model_poisson)
```


Let's run the model.

```{r}
ni = 10000
nb = 1000 

t0 = Sys.time()
nimbleMCMC_samples_poisson = nimbleMCMC(code = code_poisson,
                           data = data,
                           constants = constants, 
                           inits = inits,
                           niter = ni,
                           nburnin = nb,
                           setSeed = 1,
                           summary = TRUE,
)
t1 =  Sys.time()
t1 - t0
```


What is the summary of each estimated parameter from the binomial model?

```{r}
nimbleMCMC_samples_poisson$summary
```


Now the samples look to be very tidily centered around 0.3.

```{r}
plot(nimbleMCMC_samples_poisson$samples[ , 'beta1'], type = 'l', xlab = 'iteration',  
     ylab = expression(Beta_1))
```
