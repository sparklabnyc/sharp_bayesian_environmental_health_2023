---
title: "Introduction to Bayesian Methods"
subtitle: "SHARP Bayesian Modeling for Environmental Health Workshop"
author: "Robbie M. Parks"
date: "August 2023"
format: html
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(nimble)
library(bayesplot)
library(posterior)
library(hrbrthemes)

extrafont::loadfonts()
theme_set(theme_ipsum())

color_scheme_set(scheme = "viridis")

set.seed(2)
```

## Goal of this computing lab session

This lab will involve taking some concepts from the Bayesian methods lecture and introduce you to the way NIMBLE works.

## Introduction to NIMBLE

NIMBLE is written in a slightly unusual format if you're used to just using base `R`.
It is written in the style of a program called BUGS, which came out a few decades ago and was developed at Imperial College London.

We will start with some really basic examples to introduce the style of writing models.
These examples will feature basic regression models using linear predictors.

::: aside
These models have been adapted from [NIMBLE's examples](https://r-nimble.org/examples).
:::

### Normal-Normal example
$$
\begin{split}
y_i &\sim \text{Normal}(\mu_i, \sigma) \quad i = 1,..., N \\
\mu_i &= \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\end{split}
$$

First create some example data for our model:

```{r}
set.seed(1)

p <- 3    # number of explanatory variables
n <- 100   # number of observations
X <- matrix(round(rnorm(p * n), 2), nrow = n, ncol = p) # explanatory variables
true_betas <- c(c(0.2, 0.5, 0.3)) # coefficients
sigma <- 0.5
y <- rnorm(n, X %*% true_betas, sigma)
```

What does the dataset look like?
```{r}
df <- tibble(y = y, x1 = X[,1], x2 = X[,2], x3 = X[,3])
df
```

What does equivalent frequentist model output look like for reference?
What's the interpretation of the 95% CI?
```{r}
model_freq <- lm(
  y ~ x1 + x2 + x3,
  data = df
)
central_est <- t(t(model_freq$coefficients))
conf_int <- confint(model_freq)
cbind(central_est, conf_int)
```

Let's rewrite this model in a Bayesian framework using NIMBLE.
```{r}
code <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100)      # prior for variance components

  # regression formula
  for(i in 1:n) {
    mu[i] <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i] # manual entry of linear predictors
    y[i] ~ dnorm(mu[i], sd = sigma)
  }

})
```

Before running NIMBLE, extract data for three predictors and center around zero for better MCMC performance.
```{r}
x1 <- X[,1] - mean(X[,1])
x2 <- X[,2] - mean(X[,2])
x3 <- X[,3] - mean(X[,3])
```

Final preparation of data into lists.
```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```

Set initial values for MCMC samples
```{r}
inits <- list(alpha = 0, beta1 = 0, beta2 = 0, beta3 = 0, sigma = 1)
```

The following code will establish which samples will be used in the sampling of the posteriors.
If there is a conjugate relationship apparent between prior and posterior (e.g., Normal-Normal, Binomial-Beta, Poisson-Gamma), NIMBLE will automatically detect it here.
```{r}
model <- nimbleModel(code, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model)
```

Run the MCMC simulations
```{r}
tic = Sys.time()
nimbleMCMC_samples_initial <- nimbleMCMC(
  code = code,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000, # run 10000 samples
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc =  Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples?
```{r}
summarise_draws(nimbleMCMC_samples_initial, default_summary_measures())
summarise_draws(nimbleMCMC_samples_initial, default_convergence_measures())
```

What do the samples of one of the unknown parameters actually look like?
Let's have a look at `beta1` (which we know is 0.2).
```{r}
mcmc_trace(nimbleMCMC_samples_initial)
```

So it looks like the samples are converging quickly from the initial parameter to ~0.2. But typically we will throw some samples at the beginning to ensure that the transient samples (which is when the model samples haven't stabilized around a particular value) are not included in calculating estimates of the mean and credible intervals.
For example, you can see how the `sigma` parameter had to jump from it's initial value of 1.0 before stabilizing.
This period where we throw away samples is called the 'burn in' period.

Let's do it again but with a burn in of 1000 samples.
```{r}
tic = Sys.time()
nimbleMCMC_samples_burnin <- nimbleMCMC(
  code = code,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000,  # collect 10000 samples
  nburnin = 1000, # burn in for 1000 iterations
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc =  Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples with burn in?
```{r}
summarise_draws(nimbleMCMC_samples_burnin, default_summary_measures())
summarise_draws(nimbleMCMC_samples_burnin, default_convergence_measures())
```

Now the samples look to be very tidily centred around 0.2.
```{r}
mcmc_trace(nimbleMCMC_samples_burnin)
```

Let's try changing the priors and see if that makes any difference to how the model fits.
```{r}
code_worse_priors <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dunif(100, 100000) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100)    # prior for variance components

  # regression formula
  for(i in 1:n) {
    mu[i] <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i] # manual entry of linear predictors
    y[i] ~ dnorm(mu[i], sd = sigma)
  }
})
```

Run the MCMC simulations
```{r}
tic = Sys.time()
nimbleMCMC_samples_worse_priors <- nimbleMCMC(
  code = code_worse_priors,
  data = data,
  constants = constants,
  inits = list(alpha = 0, beta1 = 0, beta2 = 1000, beta3 = 0, sigma = 1),
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc =  Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples with different priors?
```{r}
summarise_draws(nimbleMCMC_samples_worse_priors, default_summary_measures())
summarise_draws(nimbleMCMC_samples_worse_priors, default_convergence_measures())
```
```{r}
mcmc_trace(nimbleMCMC_samples_worse_priors)
```

You can see how the model cannot recover the true value of `beta2` because of our choice of prior.
By using a uniform prior without support for the true value of `beta2`, the model has increased its variance to try and cope with the data.
This is a warning: even though the mode has converged, it might not be a good model.

Back to our sensible priors, say we wanted to establish the estimated difference between two betas?
Let's say `beta1` and `beta2` in this case.
```{r}
code_diff_betas <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100)      # prior for variance components

  # regression formula
  for(i in 1:n) {
    mu[i] <- alpha + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
    y[i] ~ dnorm(mu[i], sd = sigma)
  }

  # difference between beta1 and beta2
  beta12_diff <- beta2 - beta1
})
```

Run the MCMC simulations and specifically monitor differences between `beta1` and `beta2`.
```{r}
parameters_to_monitor = c('beta12_diff')

tic = Sys.time()
nimbleMCMC_samples_beta12_diff = nimbleMCMC(
  code = code_diff_betas,
  data = data,
  constants = constants,
  inits = inits,
  monitors = parameters_to_monitor,
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc =  Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples when monitoring difference between `beta1` and `beta2`?
```{r}
summarise_draws(nimbleMCMC_samples_beta12_diff, default_summary_measures())
summarise_draws(nimbleMCMC_samples_beta12_diff, default_convergence_measures())
```

Equally, we could do this by operating on the samples themselves from the original model.
```{r}
(nimbleMCMC_samples_burnin[, "beta2"] - nimbleMCMC_samples_burnin[, "beta1"]) |> as.numeric() |> summary()
```

### Logistic regression example.

Logistic regression is used when we want to classify observations into two groups.
$$
\begin{split}
y_i &\sim \text{Bernoulli}(p_i) \quad i = 1,..., N \\
\text{logit}(p_i) &= \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\end{split}
$$

::: aside
Note, a Bernoulli likelihood is equivalent to a Binomial with sample size 1.
:::

First create some example data for our model:
```{r}
n <- 10000
p <- 3

set.seed(1)
x1 <- round(rnorm(n), 2)
x2 <- round(rnorm(n), 2)
x3 <- round(rnorm(n), 2)

z <- 1 + 2 * x1 + 3 * x2 - 5 * x3 # linear combination with a bias
pr <- 1 / (1 + exp(-z))           # pass through an inv-logit function
y <- rbinom(n, 1, pr)             # bernoulli response variable
```

What does the dataset look like?
```{r}
df <- tibble(y = y, x1 = x1, x2 = x2, x3 = x3)
df
```

What does equivalent frequentist model output look like for reference?
```{r}
# now feed it to glm:
model_freq = glm(
  y ~ x1 + x2 + x3,
  family = "binomial",
  data = df
)

central_est = t(t(model_freq$coefficients))
conf_int = confint(model_freq)
cbind(central_est, conf_int)
```

Create the NIMBLE model
```{r}
code_binomial <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3

  # regression formula
  for(i in 1:n) {
    y[i] ~ dbin(p[i], 1)
    logit(p[i]) <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i]
  }

})
```

Before running NIMBLE, extract data for three predictors and center around zero for better MCMC performance
```{r}
x1 <- x1 - mean(x1)
x2 <- x2 - mean(x2)
x3 <- x3 - mean(x3)
```

Final preparation of data into lists
```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```

Set initial values for MCMC samples
```{r}
inits <- list(alpha = 0, beta1 = 0, beta2 = 0, beta3 = 0)
```

Let NIMBLE find out if there is a conjugate relationship between prior and posterior (e.g., Normal-Normal, Binomial-Beta, Poisson-Gamma)
```{r}
model_binomial <- nimbleModel(code_binomial, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model_binomial)
```

Let's run the model.
```{r}
tic = Sys.time()

nimbleMCMC_samples_binomial <- nimbleMCMC(
  code = code_binomial,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc =  Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the binomial model?
```{r}
summarise_draws(nimbleMCMC_samples_binomial, default_summary_measures())
summarise_draws(nimbleMCMC_samples_binomial, default_convergence_measures())
```

Now the samples for `beta1` look to be very tidily centered around 2.
```{r}
mcmc_trace(nimbleMCMC_samples_binomial)
```

### Poisson example.

$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\end{split}
$$

Again, let's make some simulated data

```{r}
n = 10000

set.seed(1)
x1 <- round(rnorm(n), 2)
x2 <- round(rnorm(n), 2)
x3 <- round(rnorm(n), 2)

z = 4 + 0.3 * x1 - 0.1 * x2 + 0.6 * x3
lambda = exp(z)
y = rpois(n, lambda)
```

What does the dataset look like?
```{r}
df <- tibble(y = y, x1 = x1, x2 = x2, x3 = x3)
df
```

What does equivalent frequentist model output look like for reference? What's the interpretation of the 95% CI?
```{r}
# now feed it to glm:
model_freq = glm(
  y ~ x1 + x2 + x3,
  family = "poisson",
  data = df
)

central_est = t(t(model_freq$coefficients))
conf_int = confint(model_freq)
cbind(central_est, conf_int)
```

Create the NIMBLE model
```{r}
code_poisson <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3

  # regression formula
  for(i in 1:n) {
    y[i] ~ dpois(lambda[i])
    log(lambda[i]) <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i]
  }

})
```

Before running NIMBLE, extract data for three predictors and center around zero for better MCMC performance
```{r}
x1 <- x1 - mean(x1)
x2 <- x2 - mean(x2)
x3 <- x3 - mean(x3)
```

Final preparation of data into lists
```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```

Set initial values for MCMC samples
```{r}
inits <- list(alpha = 0, beta1 = 0, beta2 = 0, beta3 = 0)
```

Are there any conjugate relationships?
```{r}
model_poisson <- nimbleModel(code_poisson, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model_poisson)
```

Let's run the model.
```{r}
tic = Sys.time()
nimbleMCMC_samples_poisson <- nimbleMCMC(
  code = code_poisson,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc =  Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the Poisson model?
```{r}
summarise_draws(nimbleMCMC_samples_poisson, default_summary_measures())
summarise_draws(nimbleMCMC_samples_poisson, default_convergence_measures())
```

Now the samples for `beta1` look to be very tidily centred around 0.3.
```{r}
mcmc_trace(nimbleMCMC_samples_poisson)
```

## Closing remarks

In this lab session, we have explored how to fit some basic models using Bayesian regression in NIMBLE.
We looked at the three most common likelihoods: Normal, Binomial, and Poisson.

We used simulated data here, so we had complete control over the complexity in the data.
However, in the real world, we would often try many different models if we do not know how the data were generated.
Also, not all real life examples fit the any of the three likelihoods perfectly.
For example, the data might be "overdispersed", where there is greater variability in the data than would be expected.
Luckily, there are extensions to these likelihoods that can deal with overdispersion, such as the beta-binomial likelhood for overdispersed binomial data, or the negative binomial likelihood (sometimes called the gamma-Poisson) for overdispersed Poisson data.
For more information on these likelihoods, see Chapter 12 of _Statistical Rethinking_ by Richard McElreath.
